{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Translation Project (English to Spanish)","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport random\nimport string\nimport tensorflow.strings as tf_strings\nimport tensorflow.data as tf_data\nimport re\nfrom keras.layers import TextVectorization\nimport keras\nimport tensorflow as tf\nfrom keras import layers\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:00.002071Z","iopub.execute_input":"2024-07-27T08:57:00.002487Z","iopub.status.idle":"2024-07-27T08:57:13.612268Z","shell.execute_reply.started":"2024-07-27T08:57:00.002451Z","shell.execute_reply":"2024-07-27T08:57:13.611384Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-27 08:57:01.921379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-27 08:57:01.921484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-27 08:57:02.063878: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"text_file =  keras.utils.get_file(\n    fname = \"spa-eng.zip\",\n    origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n    extract = True,\n)\ntext_file = pathlib.Path(text_file).parent/\"spa-eng\"/\"spa.txt\"\n\nwith open(text_file, \"r\") as f:\n    lines = f.read().split(\"\\n\")[:-1]\n    \ntext_pairs = []\nfor line in lines:\n    eng, spa = line.split(\"\\t\")\n    spa = \"[start] \" + spa + \" [end]\"\n    text_pairs.append((eng, spa))","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:13.614120Z","iopub.execute_input":"2024-07-27T08:57:13.614731Z","iopub.status.idle":"2024-07-27T08:57:14.048047Z","shell.execute_reply.started":"2024-07-27T08:57:13.614699Z","shell.execute_reply":"2024-07-27T08:57:14.047102Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n\u001b[1m2638744/2638744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"random.shuffle(text_pairs)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:14.049254Z","iopub.execute_input":"2024-07-27T08:57:14.049616Z","iopub.status.idle":"2024-07-27T08:57:14.169863Z","shell.execute_reply.started":"2024-07-27T08:57:14.049586Z","shell.execute_reply":"2024-07-27T08:57:14.168966Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"num_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples:]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:14.172015Z","iopub.execute_input":"2024-07-27T08:57:14.172361Z","iopub.status.idle":"2024-07-27T08:57:14.181299Z","shell.execute_reply.started":"2024-07-27T08:57:14.172306Z","shell.execute_reply":"2024-07-27T08:57:14.180249Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"118964 total pairs\n83276 training pairs\n17844 validation pairs\n17844 test pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\nvocab_size = 15000\nsequence_length = 20\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:14.182515Z","iopub.execute_input":"2024-07-27T08:57:14.182840Z","iopub.status.idle":"2024-07-27T08:57:14.192501Z","shell.execute_reply.started":"2024-07-27T08:57:14.182806Z","shell.execute_reply":"2024-07-27T08:57:14.191557Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def custom_standardization(input_string):\n    lowercase = tf_strings.lower(input_string)\n    return tf_strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\n# vectorization\neng_vectorization = TextVectorization(\n    max_tokens = vocab_size,\n    output_mode = \"int\",\n    output_sequence_length = sequence_length,\n)\n\nspa_vectorization = TextVectorization(\n    max_tokens = vocab_size,\n    output_mode = \"int\",\n    output_sequence_length = sequence_length + 1,\n    standardize = custom_standardization,\n)\n\ntrain_eng_texts = [pair[0] for pair in train_pairs]\ntrain_spa_texts = [pair[1] for pair in train_pairs]\n\neng_vectorization.adapt(train_eng_texts)\nspa_vectorization.adapt(train_spa_texts)\n\n#save the vectorization layers\neng_vectorization_config = eng_vectorization.get_config()\neng_vectorization_config.pop('standardize', None)\neng_vocab = eng_vectorization.get_vocabulary()\nwith open('eng_vectorization_config.json', 'w', encoding='utf-8') as f:\n    json.dump(eng_vectorization_config, f)\n    \nwith open('eng_vocab.json', 'w', encoding='utf-8') as f:\n    json.dump(eng_vocab, f)\n    \nspa_vectorization_config = spa_vectorization.get_config()\nspa_vectorization_config.pop('standardize', None)\nspa_vocab = spa_vectorization.get_vocabulary()\nwith open('spa_vectorization_config.json', 'w', encoding='utf-8') as f:\n    json.dump(spa_vectorization_config, f)\n    \nwith open('spa_vocab.json', 'w', encoding='utf-8') as f:\n    json.dump(spa_vocab, f)\n    \n\ndef format_dataset(eng, spa):\n    eng = eng_vectorization(eng)\n    spa = spa_vectorization(spa)\n    return (\n        {\n            \"encoder_inputs\": eng,\n            \"decoder_inputs\": spa[:, :-1],\n        },\n        spa[:, 1:],\n    )\n    \ndef make_dataset(pairs):\n    eng_texts, spa_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    spa_texts = list(spa_texts)\n    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.cache().shuffle(2048).prefetch(16)\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:14.193638Z","iopub.execute_input":"2024-07-27T08:57:14.193915Z","iopub.status.idle":"2024-07-27T08:57:17.239240Z","shell.execute_reply.started":"2024-07-27T08:57:14.193891Z","shell.execute_reply":"2024-07-27T08:57:17.238069Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for inputs,targets in train_ds.take(1):\n    print(inputs[\"encoder_inputs\"].shape)\n    print(targets.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:17.240844Z","iopub.execute_input":"2024-07-27T08:57:17.241185Z","iopub.status.idle":"2024-07-27T08:57:18.113043Z","shell.execute_reply.started":"2024-07-27T08:57:17.241155Z","shell.execute_reply":"2024-07-27T08:57:18.111979Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(64, 20)\n(64, 20)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"code","source":"# Creating an Encoder\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads = num_heads, key_dim = embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation = \"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n        \n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, None, :], dtype = tf.int32)\n        else:\n            padding_mask = None\n            \n        attention_output = self.attention(\n            query = inputs,\n            value = inputs,\n            key = inputs,\n            attention_mask = padding_mask,\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"dense_dim\": self.dense_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config\n    \n# Creating a Positional Embedding\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim = vocab_size, output_dim = embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim = sequence_length, output_dim = embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        \n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start = 0, limit = length, delta = 1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n    \n    def compute_mask(self, inputs, mask=None):\n        if mask is not None:\n            return tf.not_equal(inputs, 0)\n        else:\n            return None\n        \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"vocab_size\": self.vocab_size,\n            \"sequence_length\": self.sequence_length,\n            \"embed_dim\": self.embed_dim,\n        })\n        return config\n    \n# Creating a Decoder\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads = num_heads, key_dim = embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads = num_heads, key_dim = embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(latent_dim, activation = \"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n        \n    def call(self, inputs, encoder_outputs, mask=None):\n        casual_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, None, :], dtype = tf.int32)\n            padding_mask = tf.minimum(padding_mask, casual_mask)\n        else:\n            padding_mask = None\n            \n        attention_output_1 = self.attention_1(\n            query = inputs,\n            value = inputs,\n            key = inputs,\n            attention_mask = casual_mask,\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n        \n        attention_output_2 = self.attention_2(\n            query = out_1,\n            value = encoder_outputs,\n            key = encoder_outputs,\n            attention_mask = padding_mask,\n        )\n        \n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n        proj_output = self.dense_proj(out_2)\n        \n        return self.layernorm_3(out_2 + proj_output)\n    \n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, None]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, tf.int32)\n        mask = tf.reshape(mask,(1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [\n                tf.expand_dims(batch_size, -1),\n                tf.convert_to_tensor([1, 1]),\n            ],\n            axis = 0,\n        )\n        return tf.tile(mask, mult)\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"latent_dim\": self.latent_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:18.114748Z","iopub.execute_input":"2024-07-27T08:57:18.115094Z","iopub.status.idle":"2024-07-27T08:57:18.145094Z","shell.execute_reply.started":"2024-07-27T08:57:18.115066Z","shell.execute_reply":"2024-07-27T08:57:18.144088Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# define emmbedding dimensions, latent dimensions, and number of heads\nembed_dim = 256\nlatent_dim = 2048\nnum_heads = 8\n\n#Encoder\nencoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"encoder_inputs\")\n\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n\nencoder = keras.Model(encoder_inputs, encoder_outputs, name = \"encoder\")\n\n#Decoder\ndecoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"decoder_inputs\")\nencoder_seq_inputs = keras.Input(shape = (None, embed_dim), name = \"encoder_seq_inputs\")\n\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoder_seq_inputs)\n\nx = layers.Dropout(0.5)(x)\n\ndecoder_outputs = layers.Dense(vocab_size, activation = \"softmax\")(x)\n\ndecoder = keras.Model([decoder_inputs, encoder_seq_inputs], decoder_outputs, name = \"decoder\")\n\n# Define the final model\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\n\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name = \"transformer\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:18.146726Z","iopub.execute_input":"2024-07-27T08:57:18.147109Z","iopub.status.idle":"2024-07-27T08:57:18.938810Z","shell.execute_reply.started":"2024-07-27T08:57:18.147075Z","shell.execute_reply":"2024-07-27T08:57:18.937548Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"epochs = 20\n\ntransformer.summary()\n\ntransformer.compile(\n    \"rmsprop\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]\n)\n\ntransformer.fit(train_ds, epochs = epochs, validation_data = val_ds)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T08:57:18.941524Z","iopub.execute_input":"2024-07-27T08:57:18.941867Z","iopub.status.idle":"2024-07-27T09:13:38.777892Z","shell.execute_reply.started":"2024-07-27T08:57:18.941838Z","shell.execute_reply":"2024-07-27T09:13:38.776801Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"transformer\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,845,120\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,155,456\u001b[0m │ positional_embed… │\n│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m12,959,640\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m15000\u001b[0m)            │            │ transformer_enco… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embed… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12,959,640</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │ transformer_enco… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m   3/1302\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 44ms/step - accuracy: 0.2535 - loss: 8.9889       ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1722070655.421814     103 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1722070655.455202     103 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1722070655.477377     103 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1722070655.490826     103 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  48/1302\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:14\u001b[0m 251ms/step - accuracy: 0.5884 - loss: 5.3152","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1722070667.111882     103 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1301/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6977 - loss: 2.3021","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1722070719.454691     104 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 52ms/step - accuracy: 0.6977 - loss: 2.3015 - val_accuracy: 0.7699 - val_loss: 1.4908\nEpoch 2/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 40ms/step - accuracy: 0.7715 - loss: 1.4906 - val_accuracy: 0.9572 - val_loss: 0.3598\nEpoch 3/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 37ms/step - accuracy: 0.9478 - loss: 0.4493 - val_accuracy: 0.9843 - val_loss: 0.1515\nEpoch 4/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 37ms/step - accuracy: 0.9802 - loss: 0.1672 - val_accuracy: 0.9935 - val_loss: 0.0794\nEpoch 5/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 38ms/step - accuracy: 0.9872 - loss: 0.1127 - val_accuracy: 0.9967 - val_loss: 0.0475\nEpoch 6/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 37ms/step - accuracy: 0.9910 - loss: 0.0893 - val_accuracy: 0.9986 - val_loss: 0.0314\nEpoch 7/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 37ms/step - accuracy: 0.9951 - loss: 0.0539 - val_accuracy: 0.9992 - val_loss: 0.0191\nEpoch 8/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 36ms/step - accuracy: 0.9982 - loss: 0.0249 - val_accuracy: 0.9996 - val_loss: 0.0118\nEpoch 9/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 36ms/step - accuracy: 0.9983 - loss: 0.0214 - val_accuracy: 0.9998 - val_loss: 0.0078\nEpoch 10/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 37ms/step - accuracy: 0.9971 - loss: 0.0308 - val_accuracy: 0.9999 - val_loss: 0.0056\nEpoch 11/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 36ms/step - accuracy: 0.9997 - loss: 0.0074 - val_accuracy: 0.9999 - val_loss: 0.0040\nEpoch 12/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 36ms/step - accuracy: 0.9999 - loss: 0.0046 - val_accuracy: 0.9997 - val_loss: 0.0062\nEpoch 13/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 0.0033 - val_accuracy: 0.9999 - val_loss: 0.0019\nEpoch 14/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 0.0017 - val_accuracy: 0.9999 - val_loss: 0.0015\nEpoch 15/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9999 - val_loss: 0.0013\nEpoch 16/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 7.5344e-04 - val_accuracy: 0.9999 - val_loss: 0.0012\nEpoch 17/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 36ms/step - accuracy: 0.9986 - loss: 0.0182 - val_accuracy: 0.9999 - val_loss: 0.0012\nEpoch 18/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 36ms/step - accuracy: 0.9999 - loss: 9.5337e-04 - val_accuracy: 0.9999 - val_loss: 9.9533e-04\nEpoch 19/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 5.9135e-04 - val_accuracy: 0.9999 - val_loss: 0.0011\nEpoch 20/20\n\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 35ms/step - accuracy: 0.9999 - loss: 4.7955e-04 - val_accuracy: 0.9999 - val_loss: 9.5398e-04\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b2040093b50>"},"metadata":{}}]},{"cell_type":"code","source":"transformer.save(\"transformer_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-07-27T09:13:38.779171Z","iopub.execute_input":"2024-07-27T09:13:38.779592Z","iopub.status.idle":"2024-07-27T09:13:39.161440Z","shell.execute_reply.started":"2024-07-27T09:13:38.779560Z","shell.execute_reply":"2024-07-27T09:13:39.160381Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"spa_vocab = spa_vectorization.get_vocabulary()\nspa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\nmax_decoded_sentence_length = sequence_length\n\ndef decode_sentence(input_sentence):\n    tokenized_input_sentence = eng_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = tf.argmax(predictions[0, i, :]).numpy().item(0)\n        sampled_token = spa_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(5):\n    input_sentence = random.choice(test_eng_texts)\n    input_sentence = input_sentence.lower()\n    input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n    translated = decode_sentence(input_sentence)\n    print(f\"input: {input_sentence}\")\n    print(f\"translated: {translated}\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T09:13:39.162770Z","iopub.execute_input":"2024-07-27T09:13:39.163191Z","iopub.status.idle":"2024-07-27T09:13:49.167699Z","shell.execute_reply.started":"2024-07-27T09:13:39.163153Z","shell.execute_reply":"2024-07-27T09:13:49.166612Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"input: have you told everyone when and where the meeting will be\ntranslated: [start] que quieres debes favor yo dijo quieres película del para          \n\ninput: i love italian food\ntranslated: [start] inglés vinieron padres                 \n\ninput: i like it in boston\ntranslated: [start] estoy un el gustaría                \n\ninput: lets go back before it begins to rain\ntranslated: [start] eso va mismo un olvides de ustedes             \n\ninput: they had a lot of kids\ntranslated: [start] todo no has en atrás               \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}